{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e270b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2605fc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509d772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrayLike = np.ndarray\n",
    "\n",
    "@dataclass\n",
    "class AdamDefaults:\n",
    "    lr: float = 1e-3\n",
    "    betas: Tuple[float, float] = (0.9, 0.999)\n",
    "    eps: float = 1e-8\n",
    "    weight_decay: float = 0.0             # L2 (coupled) if decouple_wd=False; WD if decouple_wd=True\n",
    "    decouple_wd: bool = True              # AdamW by default\n",
    "    amsgrad: bool = False\n",
    "    bias_correction: bool = True\n",
    "    max_grad_norm: Optional[float] = None # global clipping (None = off)\n",
    "\n",
    "\n",
    "class _ParamAdapter:\n",
    "    \"\"\"\n",
    "    Small adapter that lets us accept either:\n",
    "      - dicts: {\"param\": ndarray, \"grad\": ndarray}\n",
    "      - objects with attributes: .data (ndarray) and .grad (ndarray or None)\n",
    "    \"\"\"\n",
    "    def __init__(self, target: Any):\n",
    "        self._t = target\n",
    "\n",
    "    @property\n",
    "    def data(self) -> ArrayLike:\n",
    "        if isinstance(self._t, dict):\n",
    "            return self._t[\"param\"]\n",
    "        return self._t.data\n",
    "\n",
    "    @property\n",
    "    def grad(self) -> Optional[ArrayLike]:\n",
    "        if isinstance(self._t, dict):\n",
    "            return self._t.get(\"grad\", None)\n",
    "        return getattr(self._t, \"grad\", None)\n",
    "\n",
    "    @grad.setter\n",
    "    def grad(self, value: Optional[ArrayLike]) -> None:\n",
    "        if isinstance(self._t, dict):\n",
    "            self._t[\"grad\"] = value\n",
    "        else:\n",
    "            setattr(self._t, \"grad\", value)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    \"\"\"\n",
    "    A lightweight, NumPy-based Adam/AdamW optimizer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : Iterable\n",
    "        Iterable of parameter carriers. Each item can be:\n",
    "          - dict {\"param\": np.ndarray, \"grad\": np.ndarray}\n",
    "          - an object with .data (np.ndarray) and .grad (np.ndarray or None)\n",
    "        You may also pass *param groups*: dictionaries with a \"params\" list and\n",
    "        optional per-group hyperparam overrides, e.g.:\n",
    "          { \"params\": [p1, p2], \"lr\": 5e-4, \"weight_decay\": 0.01 }\n",
    "    defaults : AdamDefaults\n",
    "        Global defaults. Per-group overrides take precedence.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------- public API -------------------------\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[Any],\n",
    "        defaults: AdamDefaults = AdamDefaults(),\n",
    "    ):\n",
    "        self.defaults = defaults\n",
    "        self.param_groups: List[Dict[str, Any]] = self._build_param_groups(params)\n",
    "        self.state: Dict[int, Dict[str, Any]] = {}  # per-parameter state\n",
    "        self._step: int = 0\n",
    "\n",
    "        # Initialize states lazily on first step to handle shapes properly\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Sets all gradients to zero (if present).\"\"\"\n",
    "        for group in self.param_groups:\n",
    "            for p_raw in group[\"params\"]:\n",
    "                p = _ParamAdapter(p_raw)\n",
    "                if p.grad is not None:\n",
    "                    p.grad[...] = 0.0\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "        self._step += 1\n",
    "\n",
    "        # Optional global grad clipping by norm (across all tensors in all groups)\n",
    "        if any(g[\"max_grad_norm\"] is not None for g in self.param_groups):\n",
    "            self._global_clip()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr            = group[\"lr\"]\n",
    "            beta1, beta2  = group[\"betas\"]\n",
    "            eps           = group[\"eps\"]\n",
    "            wd            = group[\"weight_decay\"]\n",
    "            decouple_wd   = group[\"decouple_wd\"]\n",
    "            amsgrad       = group[\"amsgrad\"]\n",
    "            bias_corr     = group[\"bias_correction\"]\n",
    "\n",
    "            for p_raw in group[\"params\"]:\n",
    "                p = _ParamAdapter(p_raw)\n",
    "                grad = p.grad\n",
    "                if grad is None:\n",
    "                    continue\n",
    "\n",
    "                if grad.dtype != p.data.dtype:\n",
    "                    # cast grad to param dtype (simple AMP safety)\n",
    "                    grad = grad.astype(p.data.dtype, copy=False)\n",
    "\n",
    "                state = self._get_state_for(p)\n",
    "                m, v = state[\"m\"], state[\"v\"]\n",
    "\n",
    "                if amsgrad:\n",
    "                    vhat = state[\"vhat\"]\n",
    "\n",
    "                # Apply coupled L2 regularization (Adam) by adding to the gradient\n",
    "                if wd != 0.0 and not decouple_wd:\n",
    "                    grad = grad + wd * p.data\n",
    "\n",
    "                # Update biased first/second moment estimates\n",
    "                m[...] = beta1 * m + (1.0 - beta1) * grad\n",
    "                v[...] = beta2 * v + (1.0 - beta2) * (grad * grad)\n",
    "                mhat = m / (1.0 - beta1 ** self._step)\n",
    "\n",
    "                if amsgrad:\n",
    "                    np.maximum(vhat, v, out=vhat)\n",
    "                    denom = vhat / (1 - beta2 ** self._step)\n",
    "                    denom = np.sqrt(denom) + eps\n",
    "                else:\n",
    "                    denom = v / (1 - beta2 ** self._step)\n",
    "                    denom = np.sqrt(denom) + eps\n",
    "\n",
    "                # Bias correction\n",
    "                if bias_corr:\n",
    "                    t = self._step\n",
    "                    bias_c1 = 1.0 - beta1 ** t\n",
    "                    bias_c2 = 1.0 - beta2 ** t\n",
    "                    step_size = lr * (math.sqrt(bias_c2) / bias_c1)\n",
    "                else:\n",
    "                    step_size = lr\n",
    "\n",
    "                # Parameter update\n",
    "                update = step_size * (mhat / denom)\n",
    "\n",
    "                if decouple_wd and wd != 0.0:\n",
    "                    # AdamW: decoupled weight decay\n",
    "                    p.data[...] = p.data - lr * wd * p.data - update\n",
    "                else:\n",
    "                    p.data[...] = p.data - update\n",
    "\n",
    "    def state_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Returns a Python dict with optimizer state (for checkpointing).\"\"\"\n",
    "        # Save param group settings and per-parameter states\n",
    "        packed_groups = []\n",
    "        for g in self.param_groups:\n",
    "            g_copy = {k: v for k, v in g.items() if k != \"params\"}\n",
    "            g_copy[\"param_ids\"] = [id(_ParamAdapter(p).data) for p in g[\"params\"]]\n",
    "            packed_groups.append(g_copy)\n",
    "\n",
    "        # Map states by param id\n",
    "        packed_states = {pid: self._pack_state(s) for pid, s in self._enumerate_states().items()}\n",
    "\n",
    "        return {\n",
    "            \"step\": self._step,\n",
    "            \"defaults\": self.defaults.__dict__.copy(),\n",
    "            \"param_groups\": packed_groups,\n",
    "            \"state\": packed_states,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n",
    "        \"\"\"Loads optimizer state produced by `state_dict`.\"\"\"\n",
    "        self._step = int(state_dict.get(\"step\", 0))\n",
    "        # restore defaults (non-strict—missing keys fall back to current)\n",
    "        d = state_dict.get(\"defaults\", {})\n",
    "        self.defaults = AdamDefaults(**{**self.defaults.__dict__, **d})\n",
    "\n",
    "        # Rebuild a mapping from param id to actual param\n",
    "        id_map = {id(_ParamAdapter(p).data): _ParamAdapter(p) for g in self.param_groups for p in g[\"params\"]}\n",
    "\n",
    "        # Restore per-parameter states\n",
    "        packed_states: Dict[int, Dict[str, Any]] = state_dict.get(\"state\", {})\n",
    "        self.state.clear()\n",
    "        for pid, s in packed_states.items():\n",
    "            pid = int(pid)\n",
    "            if pid in id_map:\n",
    "                p = id_map[pid]\n",
    "                self._set_state_for(p, self._unpack_state(s, p.data.shape, p.data.dtype))\n",
    "\n",
    "        # Restore group hyperparams, matching by param ids (best-effort)\n",
    "        incoming_groups: List[Dict[str, Any]] = state_dict.get(\"param_groups\", [])\n",
    "        # Build a flat list of our param ids per group for alignment\n",
    "        my_groups_ids = [[id(_ParamAdapter(p).data) for p in g[\"params\"]] for g in self.param_groups]\n",
    "        for g_in in incoming_groups:\n",
    "            in_ids = g_in.get(\"param_ids\", [])\n",
    "            # Find the best matching group by overlap\n",
    "            best = None\n",
    "            best_overlap = -1\n",
    "            for i, mine in enumerate(my_groups_ids):\n",
    "                overlap = len(set(in_ids).intersection(mine))\n",
    "                if overlap > best_overlap:\n",
    "                    best_overlap = overlap\n",
    "                    best = i\n",
    "            if best is not None and best_overlap > 0:\n",
    "                # Merge hyperparams (keep our \"params\" list)\n",
    "                for k, v in g_in.items():\n",
    "                    if k not in (\"params\", \"param_ids\"):\n",
    "                        self.param_groups[best][k] = v\n",
    "\n",
    "    # ------------------------- helpers -------------------------\n",
    "\n",
    "    def _build_param_groups(self, params: Iterable[Any]) -> List[Dict[str, Any]]:\n",
    "        # Support either a flat list of params, or list of group dicts\n",
    "        groups: List[Dict[str, Any]] = []\n",
    "        if not params:\n",
    "            raise ValueError(\"Adam received an empty params iterable.\")\n",
    "\n",
    "        def _make_group(param_list: List[Any], overrides: Dict[str, Any]) -> Dict[str, Any]:\n",
    "            # fill with defaults, then apply overrides\n",
    "            g = {\n",
    "                \"params\": list(param_list),\n",
    "                \"lr\": self.defaults.lr,\n",
    "                \"betas\": self.defaults.betas,\n",
    "                \"eps\": self.defaults.eps,\n",
    "                \"weight_decay\": self.defaults.weight_decay,\n",
    "                \"decouple_wd\": self.defaults.decouple_wd,\n",
    "                \"amsgrad\": self.defaults.amsgrad,\n",
    "                \"bias_correction\": self.defaults.bias_correction,\n",
    "                \"max_grad_norm\": self.defaults.max_grad_norm,\n",
    "            }\n",
    "            g.update(overrides)\n",
    "            # Basic validation\n",
    "            b1, b2 = g[\"betas\"]\n",
    "            if not (0.0 <= b1 < 1.0 and 0.0 <= b2 < 1.0):\n",
    "                raise ValueError(f\"Invalid betas: {g['betas']}\")\n",
    "            if g[\"lr\"] < 0.0:\n",
    "                raise ValueError(\"Invalid lr (must be >= 0).\")\n",
    "            if g[\"eps\"] <= 0.0:\n",
    "                raise ValueError(\"Invalid eps (must be > 0).\")\n",
    "            if g[\"max_grad_norm\"] is not None and g[\"max_grad_norm\"] <= 0.0:\n",
    "                raise ValueError(\"Invalid max_grad_norm (must be > 0 or None).\")\n",
    "            return g\n",
    "\n",
    "        # Normalize input\n",
    "        first = next(iter(params))\n",
    "        # Need to iterate again, so coerce to list\n",
    "        params_list = list(params) if not isinstance(params, list) else params\n",
    "\n",
    "        if isinstance(first, dict) and \"params\" in first:\n",
    "            # List of param groups\n",
    "            for g in params_list:\n",
    "                if \"params\" not in g:\n",
    "                    raise ValueError(\"Param group dicts must have a 'params' key.\")\n",
    "                groups.append(_make_group(g[\"params\"], {k: v for k, v in g.items() if k != \"params\"}))\n",
    "        else:\n",
    "            # Flat list → single group\n",
    "            groups.append(_make_group(params_list, {}))\n",
    "\n",
    "        return groups\n",
    "\n",
    "    def _get_state_for(self, p: _ParamAdapter) -> Dict[str, Any]:\n",
    "        pid = id(p.data)\n",
    "        if pid not in self.state:\n",
    "            self.state[pid] = self._new_state_like(p.data)\n",
    "        return self.state[pid]\n",
    "\n",
    "    def _set_state_for(self, p: _ParamAdapter, s: Dict[str, Any]) -> None:\n",
    "        self.state[id(p.data)] = s\n",
    "\n",
    "    def _new_state_like(self, arr: ArrayLike) -> Dict[str, Any]:\n",
    "        st = {\n",
    "            \"m\": np.zeros_like(arr, dtype=arr.dtype),\n",
    "            \"v\": np.zeros_like(arr, dtype=arr.dtype),\n",
    "        }\n",
    "        # Optional AMSGrad\n",
    "        st[\"vhat\"] = np.zeros_like(arr, dtype=arr.dtype)\n",
    "        return st\n",
    "\n",
    "    def _enumerate_states(self) -> Dict[int, Dict[str, Any]]:\n",
    "        return self.state\n",
    "\n",
    "    def _pack_state(self, s: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # Convert arrays to bytes plus shape/dtype for portability\n",
    "        def pack(a: ArrayLike) -> Dict[str, Any]:\n",
    "            return {\"shape\": a.shape, \"dtype\": str(a.dtype), \"bytes\": a.tobytes()}\n",
    "        out = {\"m\": pack(s[\"m\"]), \"v\": pack(s[\"v\"]), \"vhat\": pack(s[\"vhat\"])}\n",
    "        return out\n",
    "\n",
    "    def _unpack_state(self, s: Dict[str, Any], shape: Tuple[int, ...], dtype: np.dtype) -> Dict[str, Any]:\n",
    "        # Prefer stored dtype/shape; fall back to current param metadata\n",
    "        def unpack(packed: Dict[str, Any]) -> ArrayLike:\n",
    "            shp = tuple(packed.get(\"shape\", shape))\n",
    "            dt = np.dtype(packed.get(\"dtype\", str(dtype)))\n",
    "            arr = np.frombuffer(packed[\"bytes\"], dtype=dt).copy()\n",
    "            return arr.reshape(shp)\n",
    "        return {\"m\": unpack(s[\"m\"]), \"v\": unpack(s[\"v\"]), \"vhat\": unpack(s[\"vhat\"])}\n",
    "\n",
    "\n",
    "    def _global_clip(self) -> None:\n",
    "        \"\"\"Global norm clip across *all* grads if any group requests it.\n",
    "        Uses the smallest (most strict) max_grad_norm among groups that set it.\n",
    "        \"\"\"\n",
    "        # Determine smallest max_grad_norm among groups that enable clipping\n",
    "        max_norms = [g[\"max_grad_norm\"] for g in self.param_groups if g[\"max_grad_norm\"] is not None]\n",
    "        if not max_norms:\n",
    "            return\n",
    "        max_norm = float(min(max_norms))\n",
    "\n",
    "        # Compute global norm\n",
    "        total_sq = 0.0\n",
    "        grads: List[Tuple[_ParamAdapter, ArrayLike]] = []\n",
    "        for group in self.param_groups:\n",
    "            for p_raw in group[\"params\"]:\n",
    "                p = _ParamAdapter(p_raw)\n",
    "                if p.grad is not None:\n",
    "                    g = p.grad\n",
    "                    grads.append((p, g))\n",
    "                    total_sq += float(np.sum(g.astype(np.float64) ** 2))\n",
    "        global_norm = math.sqrt(total_sq) if total_sq > 0 else 0.0\n",
    "        if global_norm == 0.0:\n",
    "            return\n",
    "\n",
    "        # Scale if needed\n",
    "        if global_norm > max_norm:\n",
    "            scale = max_norm / (global_norm + 1e-12)\n",
    "            for p, g in grads:\n",
    "                p.grad[...] = g * scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cde652eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1257, -0.1321,  0.6404], requires_grad=True)\n",
      "tensor([ 0.1049, -0.5357,  0.3616], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Build \"parameters\"\n",
    "# x1 = {\"param\": rng.normal(size=(3,)), \"grad\": None}\n",
    "# x2 = {\"param\": rng.normal(size=(3,)), \"grad\": None}\n",
    "x1 = torch.tensor(rng.normal(size=(3,)), dtype=torch.float32, requires_grad=True)\n",
    "x2 = torch.tensor(rng.normal(size=(3,)), dtype=torch.float32, requires_grad=True)\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc7e89a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Adam.__init__() got an unexpected keyword argument 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m config_optim = {\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m:\u001b[32m1e-3\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mamsgrad\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     ],\n\u001b[32m      9\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m optimizer = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_optim\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(optimizer)\n",
      "\u001b[31mTypeError\u001b[39m: Adam.__init__() got an unexpected keyword argument 'epochs'"
     ]
    }
   ],
   "source": [
    "config_optim = {\n",
    "    \"lr\":1e-3,\n",
    "    \"amsgrad\": True,\n",
    "    \"params\": [\n",
    "        {\"params\": [x1], \"lr\": 1e-2, \"weight_decay\": 0.1, \"decoupled_weight_decay\": True},\n",
    "        {\"params\": [x2], \"lr\": 1e-2, \"weight_decay\": 0.0},\n",
    "    ],\n",
    "}\n",
    "optimizer = torch.optim.Adam(**config_optim) \n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6017198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.2955e-06,  2.8417e-06, -2.3965e-05], requires_grad=True)\n",
      "tensor([-2.0207e-06, -3.8585e-05,  1.2624e-05], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    loss = (x1**2).sum() + (x2**2).sum()\n",
    "    loss.backward()\n",
    "\n",
    "    # x1.grad = 2.0 * x1\n",
    "    # x2.grad = 2.0 * x2\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "print(x1)\n",
    "print(x2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
